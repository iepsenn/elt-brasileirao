{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Sequential()\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.20))\n",
    "# model.add(Dense(2048,activation='relu',input_dim=train.shape[1],kernel_initializer='uniform'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(2048,activation='tanh'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(1048,activation='tanh'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.25))\n",
    "# #model.add(Dense(1024,kernel_initializer='uniform',activation='tanh'))\n",
    "# #model.add(Dropout(0.5))\n",
    "# model.add(Dense(target.shape[1],activation='sigmoid'))\n",
    "\n",
    "\n",
    "# sgd=SGD(lr=0.01,momentum=0.9)\n",
    "# model.compile(optimizer=sgd,loss='binary_crossentropy',metrics=['accuracy'])\n",
    "# model.fit(train.values,target.values,batch_size=60,epochs=100,verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/serieA_matches.csv')\n",
    "df_all_matches = pd.read_csv('data/all_matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_teams = df_all_matches[['Home', 'Venue']].drop_duplicates().values\n",
    "team_stadium = {}\n",
    "for data in home_teams:\n",
    "     team_stadium[data[0]] = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_home'] = team_stadium[df['team'].iloc[0]]\n",
    "df['is_home'] = df['stadium'] == df['is_home']\n",
    "df['is_home'] = df['is_home'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "team       object\n",
       "result     object\n",
       "stadium    object\n",
       "state      object\n",
       "UF         object\n",
       "date       object\n",
       "time       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_types = df.dtypes\n",
    "cols_types[cols_types == object]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fouls</th>\n",
       "      <th>corners</th>\n",
       "      <th>crosses</th>\n",
       "      <th>touches</th>\n",
       "      <th>tackles</th>\n",
       "      <th>interceptions</th>\n",
       "      <th>aerials_won</th>\n",
       "      <th>clearances</th>\n",
       "      <th>offsides</th>\n",
       "      <th>goal_kicks</th>\n",
       "      <th>...</th>\n",
       "      <th>fouls_commited_mean_attack</th>\n",
       "      <th>fouls_drawn_mean_attack</th>\n",
       "      <th>offsides_mean_attack</th>\n",
       "      <th>crosses_mean_attack</th>\n",
       "      <th>tackles_on_mean_attack</th>\n",
       "      <th>interceptions_mean_attack</th>\n",
       "      <th>own_goals_mean_attack</th>\n",
       "      <th>penalty_kicks_won_mean_attack</th>\n",
       "      <th>penalty_kicks_conceded_mean_attack</th>\n",
       "      <th>is_home</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>455</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>590</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fouls  corners  crosses  touches  tackles  interceptions  aerials_won  \\\n",
       "0   12.0      3.0     16.0      455      7.0            9.0          8.0   \n",
       "1    7.0      4.0      8.0      590      8.0            5.0          9.0   \n",
       "\n",
       "   clearances  offsides  goal_kicks  ...  fouls_commited_mean_attack  \\\n",
       "0         6.0       1.0         6.0  ...                         1.0   \n",
       "1        13.0       1.0         8.0  ...                         0.0   \n",
       "\n",
       "   fouls_drawn_mean_attack  offsides_mean_attack crosses_mean_attack  \\\n",
       "0                      0.0                  0.00                 0.5   \n",
       "1                      1.5                  0.25                 0.5   \n",
       "\n",
       "   tackles_on_mean_attack interceptions_mean_attack own_goals_mean_attack  \\\n",
       "0                    0.00                      0.00                   0.0   \n",
       "1                    0.75                      0.25                   0.0   \n",
       "\n",
       "  penalty_kicks_won_mean_attack penalty_kicks_conceded_mean_attack  is_home  \n",
       "0                           NaN                                NaN        1  \n",
       "1                           NaN                                NaN        1  \n",
       "\n",
       "[2 rows x 119 columns]"
      ]
     },
     "execution_count": 595,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "df['time'] = df['time'].apply(lambda time: time.split(':')[0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['year'] = df['date'].dt.year\n",
    "# df['month'] = df['date'].dt.month\n",
    "# df['quarter'] = df['date'].dt.quarter\n",
    "# df['day'] = df['date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [\n",
    "    'result',\n",
    "    \"date\",\n",
    "#     'team',\n",
    "    \"stadium\",\n",
    "    \"state\",\n",
    "    \"UF\",\n",
    "#     \"match_id\"\n",
    "]\n",
    "y_col = 'score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=cols_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_dict = {}\n",
    "for idx, team in enumerate(df['team'].unique()):\n",
    "    team_dict[team] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in df.columns if col not in cols_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams = pd.DataFrame()\n",
    "for team in df['team'].unique():\n",
    "    cols_to_drop = ['is_home', 'team', 'score']\n",
    "\n",
    "    df_team = df[df['team'] == team].copy().reset_index(drop=True)\n",
    "    df_others_teams = df[\n",
    "        (df['team'] != team) &\n",
    "        (df['match_id'].isin(df_team['match_id'].tolist()))\n",
    "    ].copy().reset_index(drop=True)\n",
    "    \n",
    "    columns = [col for col in df_team if col not in cols_to_drop]\n",
    "    df_team.loc[df_team.index >= 3, columns] = (\n",
    "        df_team[columns]\n",
    "        .shift(1)\n",
    "        .rolling(3)\n",
    "        .mean()\n",
    "        .iloc[3:]\n",
    "    )\n",
    "\n",
    "    df_others_teams.loc[df_others_teams.index >= 3, columns] = (\n",
    "        df_team[columns]\n",
    "        .shift(1)\n",
    "        .rolling(3)\n",
    "        .mean()\n",
    "        .iloc[3:]\n",
    "    )\n",
    "\n",
    "    df_others_teams.drop(columns=cols_to_drop, inplace=True)\n",
    "    df_others_teams.columns = [\n",
    "        col + '_adversary' if col != 'match_id' else col for col in df_others_teams.columns\n",
    "    ]\n",
    "    \n",
    "    df_teams = df_teams.append(\n",
    "        df_team.merge(df_others_teams, on='match_id')\n",
    "    )\n",
    "df_teams['team'] = df_teams['team'].apply(lambda team: team_dict[team])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teams['score'].fillna(-1, inplace=True)\n",
    "df_teams = df_teams[df_teams['score'] != -1]\n",
    "df_teams['score'] = df_teams['score'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_teams.drop(columns=['score'])\n",
    "Y = df_teams['score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score\n",
    "# result\n",
    "\n",
    "# df.drop(columns=['result'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(columns=cols_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = XGBClassifier()\n",
    "\n",
    "# rfe = RFE(model, 25)\n",
    "# fit = rfe.fit(X, Y)\n",
    "# print(\"Num Features: %d\" % fit.n_features_)\n",
    "# print(\"Selected Features: %s\" % fit.support_)\n",
    "# print(\"Feature Ranking: %s\" % fit.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest\n",
    "# from sklearn.feature_selection import f_classif\n",
    "\n",
    "# test = SelectKBest(score_func=f_classif, k=25)\n",
    "# fit = test.fit(X, Y)\n",
    "# # summarize scores\n",
    "# print(fit.scores_)\n",
    "# features = fit.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = np.array(np.array(X.columns.tolist()))[list(fit.support_)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    difference = list(np.abs((y_true - y_pred) / y_true))\n",
    "    difference = [diff if diff != np.inf and not pd.isnull(diff) else 0 for diff in difference]\n",
    "    return np.mean(difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_percentage(y_true, y_pred): \n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    return  np.sum(y_pred) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sort_values(['matchweek', 'team'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py:67: FutureWarning: Pass n_features_to_select=50 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "# n_cols_list = [5, 15, 25, 35, 45, 50, 63, 72, 84, 90, 100]\n",
    "# for n_cols in n_cols_list:\n",
    "n_cols = 50\n",
    "\n",
    "model = XGBRegressor()\n",
    "\n",
    "rfe = RFE(model, n_cols)\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "cols = np.array(np.array(X.columns.tolist()))[list(fit.support_)]\n",
    "X_new = X[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "parameters = {\n",
    "    'eta': [0.05, 0.1, 0.3],\n",
    "    'max_depth': [5, 6, 9, 12, 20],\n",
    "    'subsample': [0.7, 0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.3, 0.6, 0.8, 1.0],\n",
    "    'min_child_weight': [1, 5, 8, 10],\n",
    "    'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "    'n_estimators': [50, 100, 150, 500, 1000]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(model, parameters, n_jobs=1, cv=3)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = clf.best_params_\n",
    "\n",
    "model = XGBRegressor(**best_parameters)\n",
    "# model = XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = list(model.predict(X_test))\n",
    "predictions = list([round(value) for value in y_pred])\n",
    "\n",
    "# evaluate predictions\n",
    "mape = mean_absolute_percentage_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "bias_percent = bias_percentage(y_test, predictions)\n",
    "\n",
    "print(\"MAPE with {} columns: {:.2f}%\".format(n_cols, mape * 100.0))\n",
    "print(\"MAE with {} columns: {:.2f}\".format(n_cols, mae))\n",
    "print(\"BIAS% with {} columns: {:.2f}%\".format(n_cols, bias_percent * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_test1 = {\n",
    "#  'max_depth':range(3,10,2),\n",
    "#  'min_child_weight':range(1,6,2)\n",
    "# }\n",
    "# gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,\n",
    "#  min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "#  objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), \n",
    "#  param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\n",
    "# gsearch1.fit(train[predictors],train[target])\n",
    "# gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_per_round = df['matchweek'].value_counts().to_frame()\n",
    "last_matchweek = match_per_round[match_per_round > 5].sort_values('matchweek', ascending=False).iloc[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_matches = pd.read_csv('data/all_matches.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_matches['Score'] = next_matches['Score'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_matches = next_matches[next_matches['Score'] == '']\n",
    "next_matches['Wk'] = next_matches['Wk'].fillna('')\n",
    "next_matches = next_matches[next_matches['Wk'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_matches = next_matches[next_matches['Wk'] > last_matchweek]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for next_match in next_matches.iterrows():\n",
    "    next_match = next_match[1].to_frame().transpose()\n",
    "    team1, team2 = next_match[['Home', 'Away']].iloc[0]\n",
    "    date = pd.to_datetime(next_match['Date'].iloc[0])\n",
    "    time = next_match['Time'].iloc[0].split(':')[0]\n",
    "    matchweek = int(next_match['Wk'].iloc[0])\n",
    "    \n",
    "    if matchweek > last_matchweek + 1:\n",
    "        break\n",
    "        \n",
    "    cols = X_new.columns.tolist()\n",
    "    mean_features_team1 = df_teams[df_teams['team'] == team_dict[team1]][cols]\n",
    "    mean_features_team2 = df_teams[df_teams['team'] == team_dict[team2]][cols]\n",
    "    \n",
    "    for col in cols:\n",
    "        mean_features_team1[col] = mean_features_team1[col].shift(1).rolling(window=3).mean()\n",
    "        mean_features_team2[col] = mean_features_team2[col].shift(1).rolling(window=3).mean()\n",
    "\n",
    "    mean_features_team1 = mean_features_team1.iloc[-1:]\n",
    "    mean_features_team2 = mean_features_team2.iloc[-1:]\n",
    "    \n",
    "    game_prediction = mean_features_team1.append(mean_features_team2)\n",
    "    prediction = [round(prediction) for prediction in list(model.predict(game_prediction))]\n",
    "    print(f\"Rodada {matchweek}: {team1} {prediction[0]} x {prediction[1]} {team2}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
